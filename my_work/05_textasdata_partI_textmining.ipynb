{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load packages \n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "## nltk imports\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "## uncomment and download if this is your first \n",
    "## time running \n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "## sentiment analysis\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "## specify to print all output in a call\n",
    "## and not just first\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## spacy --- if you get an error at the load step\n",
    "## need to download en_core_web_sm (google)\n",
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>name_upper</th>\n",
       "      <th>neighbourhood_group</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2539</td>\n",
       "      <td>Clean &amp; quiet apt home by the park</td>\n",
       "      <td>CLEAN &amp; QUIET APT HOME BY THE PARK</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2595</td>\n",
       "      <td>Skylit Midtown Castle</td>\n",
       "      <td>SKYLIT MIDTOWN CASTLE</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3647</td>\n",
       "      <td>THE VILLAGE OF HARLEM....NEW YORK !</td>\n",
       "      <td>THE VILLAGE OF HARLEM....NEW YORK !</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3831</td>\n",
       "      <td>Cozy Entire Floor of Brownstone</td>\n",
       "      <td>COZY ENTIRE FLOOR OF BROWNSTONE</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5022</td>\n",
       "      <td>Entire Apt: Spacious Studio/Loft by central park</td>\n",
       "      <td>ENTIRE APT: SPACIOUS STUDIO/LOFT BY CENTRAL PARK</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                              name  \\\n",
       "0  2539                Clean & quiet apt home by the park   \n",
       "1  2595                             Skylit Midtown Castle   \n",
       "2  3647               THE VILLAGE OF HARLEM....NEW YORK !   \n",
       "3  3831                   Cozy Entire Floor of Brownstone   \n",
       "4  5022  Entire Apt: Spacious Studio/Loft by central park   \n",
       "\n",
       "                                         name_upper neighbourhood_group  price  \n",
       "0                CLEAN & QUIET APT HOME BY THE PARK            Brooklyn    149  \n",
       "1                             SKYLIT MIDTOWN CASTLE           Manhattan    225  \n",
       "2               THE VILLAGE OF HARLEM....NEW YORK !           Manhattan    150  \n",
       "3                   COZY ENTIRE FLOOR OF BROWNSTONE            Brooklyn     89  \n",
       "4  ENTIRE APT: SPACIOUS STUDIO/LOFT BY CENTRAL PARK           Manhattan     80  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48895 entries, 0 to 48894\n",
      "Data columns (total 5 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   id                   48895 non-null  int64 \n",
      " 1   name                 48879 non-null  object\n",
      " 2   name_upper           48879 non-null  object\n",
      " 3   neighbourhood_group  48895 non-null  object\n",
      " 4   price                48895 non-null  int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "## if working from within the repo, can use this relative path\n",
    "path_todata = \"../public_data/airbnb_text.csv\"\n",
    "\n",
    "## load data\n",
    "ab = pd.read_csv(path_todata)\n",
    "ab.head()\n",
    "ab.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual approach 1: look for a single word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_mention_cozy</th>\n",
       "      <th>mention_cozy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bronx</th>\n",
       "      <td>89.231088</td>\n",
       "      <td>74.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brooklyn</th>\n",
       "      <td>128.175441</td>\n",
       "      <td>91.130224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Manhattan</th>\n",
       "      <td>204.109775</td>\n",
       "      <td>129.917140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Queens</th>\n",
       "      <td>102.596682</td>\n",
       "      <td>80.344388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Staten Island</th>\n",
       "      <td>120.650307</td>\n",
       "      <td>74.319149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     no_mention_cozy  mention_cozy\n",
       "neighbourhood_group                               \n",
       "Bronx                      89.231088     74.214286\n",
       "Brooklyn                  128.175441     91.130224\n",
       "Manhattan                 204.109775    129.917140\n",
       "Queens                    102.596682     80.344388\n",
       "Staten Island             120.650307     74.319149"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## using the `name_upper` var, look at where reviews mention cozy\n",
    "ab['is_cozy'] = np.where(ab.name_upper.str.contains(\"COZY\"), True, False)\n",
    "\n",
    "## find the mean price by neighborhood and whether cozy\n",
    "mp = pd.DataFrame(ab.groupby(['is_cozy', 'neighbourhood_group'])['price'].mean())\n",
    "\n",
    "## reshape to wide format so that each borough is row\n",
    "## and one col is the mean price for listings that describe\n",
    "## the place as cozy; other col is mean price for listings\n",
    "## without that word\n",
    "mp_wide = pd.pivot_table(mp, index = ['neighbourhood_group'],\n",
    "                        columns = ['is_cozy'])\n",
    "\n",
    "mp_wide.columns = ['no_mention_cozy', 'mention_cozy']\n",
    "\n",
    "mp_wide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual approach 2: score based on dictionary of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COZY', 'LITTLE']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## construct dictionary\n",
    "space_indicators = {'small': ['COZY', 'COMFY', 'LITTLE', 'SMALL'],\n",
    "                   'large': ['SPACIOUS', 'LARGE', 'HUGE', 'GIANT']}\n",
    "\n",
    "\n",
    "## for each listing, find the number of occurrences\n",
    "## of words in each key\n",
    "\n",
    "### first, let's test with one listing\n",
    "practice_listing = \"NICE AND COZY LITTLE APT AVAILABLE\"\n",
    "\n",
    "### splitting that string at space and looking at overlap with each key\n",
    "### first, look at overlap with the list containing words for small\n",
    "words_overlap_small = [word \n",
    "                    for word in practice_listing.split(\" \") if \n",
    "                      word in space_indicators['small']]\n",
    "words_overlap_small\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### then, look at overlap with the list containing words for large\n",
    "words_overlap_large = [word for word in practice_listing.split(\" \") if \n",
    "                      word in space_indicators['large']]\n",
    "words_overlap_large\n",
    "\n",
    "### could then take length as a fraction of all words\n",
    "len(words_overlap_small)/len(practice_listing.split(\" \"))\n",
    "len(words_overlap_large)/len(practice_listing.split(\" \"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a chill apt next to the subway in LES Chinatown'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## specify example\n",
    "example_for_tag = \"This is a chill apt next to the subway in LES Chinatown\"\n",
    "example_for_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('chill', 'NN'),\n",
       " ('apt', 'JJ'),\n",
       " ('next', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('subway', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('LES', 'NNP'),\n",
       " ('Chinatown', 'NNP')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## try part of speech tagging using nltk\n",
    "tokens = word_tokenize(example_for_tag) # Generate list of tokens\n",
    "tokens_pos = pos_tag(tokens) # generate part of speech tags for those tokens\n",
    " \n",
    "## returns a list of tuples\n",
    "## first element in tuple is a word\n",
    "## second element in tuple is the part of speech\n",
    "#for one_tok in tokens_pos:\n",
    " #   print(one_tok)\n",
    "tokens_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LES', 'Chinatown']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['chill', 'apt', 'next', 'subway']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## use list iteration to extract proper nouns (NNP)\n",
    "## i'm first checking if the second element in the tuple\n",
    "## is equal to NNP\n",
    "## if so, i'm returning the first element in the tuple (the \n",
    "## actual word)\n",
    "all_prop_noun = [one_tok[0] for one_tok in tokens_pos \n",
    "                if one_tok[1] == \"NNP\"]\n",
    "all_prop_noun\n",
    "\n",
    "all_adj_noun = [one_tok[0] for one_tok in tokens_pos \n",
    "                if one_tok[1] == \"JJ\" or \n",
    "               one_tok[1] == \"NN\"]\n",
    "all_adj_noun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## modified from a real tweet\n",
    "\n",
    "## tweet\n",
    "d_tweet = \"\"\"Weâ€™ll be hosting on-campus COVID-19 booster clinics \n",
    "at Dartmouth College in New Hampshire from 9 a.m. to 6 p.m. on\n",
    "Monday, Jan. 10, and Tuesday, Jan. 11, at\n",
    "Alumni Hall in the Hopkins Center. For information on how to\n",
    "register and additional winter updates, head to\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "spacy_dtweet = nlp(d_tweet)\n",
    "print(type(spacy_dtweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: COVID-19; NER tag: ORG\n",
      "Entity: Dartmouth College; NER tag: ORG\n",
      "Entity: New Hampshire; NER tag: GPE\n",
      "Entity: 9 a.m. to 6 p.m.; NER tag: TIME\n",
      "Entity: Monday, Jan. 10; NER tag: DATE\n",
      "Entity: Tuesday, Jan. 11; NER tag: DATE\n",
      "Entity: Alumni Hall; NER tag: FAC\n",
      "Entity: the Hopkins Center; NER tag: FAC\n"
     ]
    }
   ],
   "source": [
    "## try a couple variations\n",
    "for one_tok in spacy_dtweet.ents:\n",
    "    print(\"Entity: \" + one_tok.text + \"; NER tag: \" + one_tok.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1\n",
    "\n",
    "Play around with different variations of the Dartmouth tweet and look at the results. For instance, try the following:\n",
    "\n",
    "- What happens if you abbreviate New Hampshire to NH?\n",
    "- What happens if you add the word Pfizer before COVID-19?\n",
    "- What entities seem misclassified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COVID-19',\n",
       " 'Dartmouth College',\n",
       " 'nh',\n",
       " '9 a.m. to 6 p.m.',\n",
       " 'Monday, Jan. 10',\n",
       " 'Tuesday, Jan. 11',\n",
       " 'Alumni Hall',\n",
       " 'the Hopkins Center']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "[re.sub(r\"New Hampshire\", \"nh\", one_tok.text) for one_tok in spacy_dtweet.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pfizer COVID-19',\n",
       " 'Dartmouth College',\n",
       " 'New Hampshire',\n",
       " '9 a.m. to 6 p.m.',\n",
       " 'Monday, Jan. 10',\n",
       " 'Tuesday, Jan. 11',\n",
       " 'Alumni Hall',\n",
       " 'the Hopkins Center']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[re.sub(r\"COVID-19\", \"Pfizer COVID-19\", one_tok.text) for one_tok in spacy_dtweet.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2\n",
    "\n",
    "How do we generalize from doing Named Entity Recognition one just one string to doing NER on a whole column that contains strings? By defining and executing a **function**, of course!\n",
    "\n",
    "Using the following sample of strings from airbnb listing names, define a function that takes in one airbnb string and does the following:\n",
    "\n",
    "- iterates over entities in that string (list comprehension would work well for this)\n",
    "- checks if each label indicates a place\n",
    "- returns the text of each original place/entity\n",
    "\n",
    "Then execute the function on the example strings through iteration (again, list comprehension works well here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10                    Beautiful 1br on Upper West Side\n",
       "11                     Central Manhattan/near Broadway\n",
       "12      Lovely Room 1, Garden, Best Area, Legal rental\n",
       "13    Wonderful Guest Bedroom in Manhattan for SINGLES\n",
       "14                       West Village Nest - Superhost\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## for runtime purposes, take a sample of the airbnb listing names\n",
    "ab_name_examples = ab.name[10:15]\n",
    "ab_name_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'spacy.tokens.token.Token' object has no attribute 'label_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Return the list of place names found\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m places\n\u001b[0;32m---> 15\u001b[0m ab_name_examples\u001b[38;5;241m.\u001b[39mapply(extract_places_from_listing)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:4764\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4630\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4631\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4636\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4637\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4638\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4639\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4640\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4755\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4756\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[1;32m   4758\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4759\u001b[0m         func,\n\u001b[1;32m   4760\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[1;32m   4761\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[1;32m   4762\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   4763\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m-> 4764\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:1209\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:1289\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1287\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1288\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1289\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[1;32m   1290\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[1;32m   1291\u001b[0m )\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1295\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[1;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1818\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2926\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[32], line 10\u001b[0m, in \u001b[0;36mextract_places_from_listing\u001b[0;34m(listing_name)\u001b[0m\n\u001b[1;32m      7\u001b[0m entities \u001b[38;5;241m=\u001b[39m nlp(listing_name)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Iterate over each entity using list comprehension to filter and collect place names\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m places \u001b[38;5;241m=\u001b[39m [entity\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m entity \u001b[38;5;129;01min\u001b[39;00m entities \u001b[38;5;28;01mif\u001b[39;00m entity\u001b[38;5;241m.\u001b[39mlabel_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLOC\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Return the list of place names found\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m places\n",
      "Cell \u001b[0;32mIn[32], line 10\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m entities \u001b[38;5;241m=\u001b[39m nlp(listing_name)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Iterate over each entity using list comprehension to filter and collect place names\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m places \u001b[38;5;241m=\u001b[39m [entity\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m entity \u001b[38;5;129;01min\u001b[39;00m entities \u001b[38;5;28;01mif\u001b[39;00m entity\u001b[38;5;241m.\u001b[39mlabel_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLOC\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Return the list of place names found\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m places\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'spacy.tokens.token.Token' object has no attribute 'label_'"
     ]
    }
   ],
   "source": [
    "# your function code here\n",
    "def extract_places_from_listing(listing_name):\n",
    "    # Initialize an empty list to hold the names of places found\n",
    "    places = []\n",
    "    \n",
    "    # Use an entity recognition tool to find entities in the listing_name\n",
    "    entities = nlp(listing_name)\n",
    "    \n",
    "    # Iterate over each entity using list comprehension to filter and collect place names\n",
    "    places = [entity.text for entity in entities if entity.label_ == \"LOC\"]\n",
    "    \n",
    "    # Return the list of place names found\n",
    "    return places\n",
    "\n",
    "ab_name_examples.apply(extract_places_from_listing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the default scorer on a few example phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize a scorer\n",
    "sent_obj = SentimentIntensityAnalyzer()\n",
    "print(type(sent_obj))\n",
    "## score one listing\n",
    "practice_listing = \"NICE AND COZY LITTLE APT AVAILABLE\"\n",
    "sentiment_example = sent_obj.polarity_scores(practice_listing)\n",
    "sentiment_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adding phrase with word terrible and score\n",
    "practice_listing_2 = \"NICE AND COZY LITTLE APT AVAILABLE. REALLY TERRIBLE VIEW.\"\n",
    "sentiment_example_2 = sent_obj.polarity_scores(practice_listing_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adding phrase about rats; bad but might not be in scoring dictionary\n",
    "practice_listing_3 = \"NICE AND COZY LITTLE APT AVAILABLE. HAS RATS THOUGH.\"\n",
    "sentiment_example_3 = sent_obj.polarity_scores(practice_listing_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## summarize all 3\n",
    "print(\"String: \" + practice_listing + \" scored as:\\n\" + str(sentiment_example))\n",
    "print(\"String: \" + practice_listing_2 + \" scored as:\\n\" + str(sentiment_example_2))\n",
    "print(\"String: \" + practice_listing_3 + \" scored as:\\n\" + str(sentiment_example_3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the dictionary with manually-added words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(sent_obj.lexicon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lexicon is a dictionary where the key\n",
    "## is the word\n",
    "## the value is the score (negative = negative)\n",
    "## here, i'm benchmarking the negativity of the\n",
    "## rodents to the negativity of the word aversion\n",
    "sent_obj.lexicon['aversion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a dictionary with \n",
    "## negative scores for pests\n",
    "pest_words = {\n",
    "    'rat': -1.9,\n",
    "    'rats': -1.9,\n",
    "    'mice': -1.9,\n",
    "    'mouse': -1.9,\n",
    "    'roach': -1.9,\n",
    "    'cockroach': -1.9\n",
    "}\n",
    "\n",
    "\n",
    "## initiate new sentiment object\n",
    "## so that we don't alter old one\n",
    "## use.update to add new words\n",
    "new_si = SentimentIntensityAnalyzer()\n",
    "new_si.lexicon.update(pest_words)\n",
    "\n",
    "## try re-scoring the third example\n",
    "## see negative\n",
    "print(\"After lexicon update: \" + practice_listing_3 + \" scored as:\\n\" + \\\n",
    "      str(new_si.polarity_scores(practice_listing_3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
